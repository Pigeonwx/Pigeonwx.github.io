# 总览

![](https://upload-images.jianshu.io/upload_images/1667471-37315f7baaee75f4.jpg)

# 数据处理

## 数据噪音

在自然语言处理（NLP）领域，数据噪音通常是指数据集中存在的错误、不准确或不相关的信息，它们可能会影响模型的性能和结果。数据噪音包括但不限于以下几种类型：

1. **拼写错误和语法错误**：这些是文本数据中常见的噪音类型，特别是在社交媒体或用户生成内容中。例如，“teh”代替“the”或者“i am go to store”代替“i am going to the store”。

2. **标点符号错误**：缺少标点符号或使用不正确的标点符号会导致理解困难。例如，“Lets eat, grandma”与“Lets eat grandma”语义完全不同。

3. **重复和冗余数据**：重复的句子或段落会引入噪音，使得模型在训练过程中关注重复的信息，而不是有意义的变化部分。

4. **表情符号和特殊字符**：这些在某些情境下可能有用，但在大多数语言任务中它们被视为噪音。例如，在情感分析中，表情符号可能有信息价值，但在翻译任务中它们通常是噪音。

5. **音同异义词**：这些词在口语中容易混淆。例如，"there"、"their" 和 "they're"。

6. **多语言混杂**：在多语言环境中，不同语言的混合使用可能会造成难以解析的情况。

7. **方言和俚语**：不同地域和群体的语言习惯可能不同，标准语言模型难以处理这些变异。

8. **错误标签**：在监督学习过程中，训练数据中的错误标签会导致模型学习到错误的信息。

9. **不相关内容**：在某些任务中（如文本分类），数据集中可能包含与任务无关的部分，这些部分会干扰模型的学习过程。

10. **错别字和打字错误**：这些错误在用户生成内容中尤其常见，对 NLP 任务如情感分析或语义理解有直接影响。

数据噪音的存在会降低模型的准确性和可靠性，因此清洗和预处理数据是 NPL 项目的关键步骤之一。通过减少噪音，可以提高模型的性能，使其能够在真实世界应用中取得更好的结果。



## 数据清洗和预处理

在自然语言处理（NLP）项目中，数据清洗和预处理是非常关键的步骤。这些步骤可以帮助提高模型的性能和准确性。以下是一些常见的数据清洗和预处理方法：

1. **去除标点符号**：
   - 删除句子中所有标点符号，可以简化文本处理工作。
   - 例如，"Hello, world!" -> "Hello world"

2. **转为小写字母**：
   - 将所有文本转换为小写，以减少词汇表的大小。
   - 例如，"Hello World" -> "hello world"

3. **去除停用词**：
   - 停用词（如“the”、“is”、“at”等）在很多 NLP 任务中无意义，可以选择去除。
   - 使用预定义的停用词表，或者根据具体任务定制。

4. **去除特殊字符和数字**：
   - 删除特殊字符（如“@”、'#'等）和数字，以保持文本简洁。
   - 某些任务可能需要保留特定字符（如情感分析中的表情符号）。

5. **词干还原和词形还原**：
   - **词干还原（Stemming）**：将词语变化还原到词干形式。
     - 例如，"running", "ran", "runs" -> "run"
   - **词形还原（Lemmatization）**：将词语还原到其基本词形。
     - 例如，"better" -> "good"（词形还原能够处理更复杂的情况）

6. **句子分割和分词**：
   - **句子分割**：将文本段落按照句子分割。
     - 例如，"Hello. How are you?" -> ["Hello.", "How are you?"]
   - **分词**：将句子分割成单词。
     - 例如，"Hello world" -> ["Hello", "world"]

7. **去除空白和重复内容**：
   - 删除多余空格和重复的单词、句子或段落。
   - 例如，”  Hello   world  ” -> "Hello world"

8. **拼写检查和更正**：
   - 识别并更正拼写错误和打字错误。
   - 使用拼写检查工具或库（如 Pyspellchecker）。

9. **处理缩写和俚语**：
   - 扩展缩写词语，替换俚语为标准词。
   - 例如，"aren't" -> "are not", "gonna" -> "going to"

10. **多语言处理**：
    - 识别并处理混杂的多语言内容，有时需要特定的语言检测工具。

11. **向量化**：
    - 将文本数据转换为数值形式，以便于机器学习模型处理。
    - 常见方法包括词袋模型（Bag of Words）、TF-IDF、Word2Vec、GloVe、BERT 等。

12. **去除或处理罕见词**：
    - 根据具体任务，考虑是否删除或合并那些出现频次极低的词。
    - 常用的方法是设置词频阈值，仅保留频次高于该阈值的词。

13. **句法和语义分析**：
    - 使用句法分析工具（如依存句法分析树）理解句子结构。
    - 使用命名实体识别（NER）标注特定实体，如人名、地名、组织等。

这些方法可以单独使用，也可以组合使用，并根据具体的应用和任务进行调整。良好的数据清洗和预处理是构建高性能 NLP 模型的基础。

## Word Embedding

⾃然语⾔是⼀套⽤来表达含义的复杂系统。在这套系统中，词是表义的基本单元。顾名思义，词向量是⽤来表⽰词的向量，也可被认为是词的特征向量或表征。**把词映射为实数域向量的技术也叫词嵌⼊（word embedding）。**近年来，词嵌⼊已逐渐成为⾃然语⾔处理的基础知识。

在NLP(自然语言处理)领域，文本表示是第一步，也是很重要的一步，通俗来说就是把人类的语言符号转化为机器能够进行计算的数字，因为普通的文本语言机器是看不懂的，必须通过转化来表征对应文本。早期是**基于规则**的方法进行转化，而现代的方法是**基于统计机器学习**的方法。

**数据决定了机器学习的上限,而算法只是尽可能逼近这个上限，**在本文中数据指的就是文本表示，所以，弄懂文本表示的发展历程，对于NLP学习者来说是必不可少的。接下来开始我们的发展历程。文本表示分为**离散表示**和**分布式表示**.

### 离散表示

#### One-hot表示

One-hot简称读热向量编码，也是特征工程中最常用的方法。其步骤如下：

1. 构造文本分词后的字典，每个分词是一个比特值，比特值为0或者1。
2. 每个分词的文本表示为该分词的比特位为1，其余位为0的矩阵表示。

例如：**John likes to watch movies. Mary likes too**

**John also likes to watch football games.**

以上两句可以构造一个词典，**{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, "also": 6, "football": 7, "games": 8, "Mary": 9, "too": 10} **

每个词典索引对应着比特位。那么利用One-hot表示为：

**John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] **

**likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]** .......等等，以此类推。

One-hot表示文本信息的**缺点**：

- 随着语料库的增加，数据特征的维度会越来越大，产生一个维度很高，又很稀疏的矩阵。
- 这种表示方法的分词顺序和在句子中的顺序是无关的，不能保留词与词之间的关系信息。



#### 词袋模型

词袋模型(Bag-of-words model)，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。

**文档的向量表示可以直接将各词的词向量表示加和**。例如：

**John likes to watch movies. Mary likes too**

**John also likes to watch football games.**

以上两句可以构造一个词典，**{"John": 1, "likes": 2, "to": 3, "watch": 4, "movies": 5, "also": 6, "football": 7, "games": 8, "Mary": 9, "too": 10} **

那么第一句的向量表示为：**[1,2,1,1,1,0,0,0,1,1]**，其中的2表示**likes**在该句中出现了2次，依次类推。

词袋模型同样有一下**缺点**：

- 词向量化后，词与词之间是有大小关系的，不一定词出现的越多，权重越大。
- 词与词之间是没有顺序关系的。



#### TF-IDF

TF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。TF意思是词频(Term Frequency)，IDF意思是逆文本频率指数(Inverse Document Frequency)。

**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。**

![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-21_10-7-23.png)

分母之所以加1，是为了避免分母为0。

那么，![](https://latex.codecogs.com/gif.latex?TF-IDF=TF*IDF)，从这个公式可以看出，当w在文档中出现的次数增大时，而TF-IDF的值是减小的，所以也就体现了以上所说的了。

**缺点：**还是没有把词与词之间的关系顺序表达出来。

#### n-gram模型

n-gram模型为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是滑窗的大小，例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式进行编码得到结果。改模型考虑了词的顺序。

例如：

**John likes to watch movies. Mary likes too**

**John also likes to watch football games.**

以上两句可以构造一个词典，**{"John likes”: 1, "likes to”: 2, "to watch”: 3, "watch movies”: 4, "Mary likes”: 5, "likes too”: 6, "John also”: 7, "also likes”: 8, “watch football”: 9, "football games": 10}**

那么第一句的向量表示为：**[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]**，其中第一个1表示**John likes**在该句中出现了1次，依次类推。

**缺点：**随着n的大小增加，词表会成指数型膨胀，会越来越大。

#### 离散表示存在的问题

由于存在以下的问题，对于一般的NLP问题，是可以使用离散表示文本信息来解决问题的，但对于要求精度较高的场景就不适合了。

- 无法衡量词向量之间的关系。
- 词表的维度随着语料库的增长而膨胀。
- n-gram词序列随语料库增长呈指数型膨胀，更加快。
- 离散数据来表示文本会带来数据稀疏问题，导致丢失了信息，与我们生活中理解的信息是不一样的。



### 分布式表示

科学家们为了提高模型的精度，又发明出了分布式的表示文本信息的方法，这就是这一节需要介绍的。**用一个词附近的其它词来表示该词，这是现代统计自然语言处理中最有创见的想法之一。**当初科学家发明这种方法是基于人的语言表达，认为一个词是由这个词的周边词汇一起来构成精确的语义信息。就好比，物以类聚人以群分，如果你想了解一个人，可以通过他周围的人进行了解，因为周围人都有一些共同点才能聚集起来。



#### 共现矩阵

共现矩阵顾名思义就是共同出现的意思，词文档的共现矩阵主要用于发现主题(topic)，用于主题模型，如LSA。

局域窗中的word-word共现矩阵可以挖掘语法和语义信息，**例如：**

- I like deep learning.	
- I like NLP.	
- I enjoy flying

有以上三句话，设置滑窗为2，可以得到一个词典：**{"I like","like deep","deep learning","like NLP","I enjoy","enjoy flying","I like"}**。

我们可以得到一个共现矩阵(对称矩阵)：

![68747470733a2f2f7778322e73696e61696d672e636e2f6c617267652f30303633304465666c79316732727776316f70357a6a333071373063377768322e6a7067](/Users/xiangjianhang/init-git/pigeonwx.github.io/docs/AI/nlp/68747470733a2f2f7778322e73696e61696d672e636e2f6c617267652f30303633304465666c79316732727776316f70357a6a333071373063377768322e6a7067.jpeg)

中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了**共现**的特性。

**存在的问题：**

- 向量维数随着词典大小线性增长。
- 存储整个词典的空间消耗非常大。
- 一些模型如文本分类模型会面临稀疏性问题。
- **模型会欠稳定，每新增一份语料进来，稳定性就会变化。**

### 神经网络表示

#### NNLM

NNLM (Neural Network Language model)，神经网络语言模型是03年提出来的，通过训练得到中间产物--词向量矩阵，这就是我们要得到的文本表示向量矩阵。

NNLM说的是定义一个前向窗口大小，其实和上面提到的窗口是一个意思。把这个窗口中最后一个词当做y，把之前的词当做输入x，通俗来说就是预测这个窗口中最后一个词出现概率的模型。

以下是NNLM的网络结构图：

![68747470733a2f2f7778332e73696e61696d672e636e2f6c617267652f30303633304465666c79316732743166346271696c6a33306c7630653261646c2e6a7067](/Users/xiangjianhang/init-git/pigeonwx.github.io/docs/AI/nlp/68747470733a2f2f7778332e73696e61696d672e636e2f6c617267652f30303633304465666c79316732743166346271696c6a33306c7630653261646c2e6a7067.jpeg)

- input层是一个前向词的输入，是经过one-hot编码的词向量表示形式，具有V*1的矩阵。

- C矩阵是投影矩阵，也就是稠密词向量表示，在神经网络中是**w参数矩阵**，该矩阵的大小为D*V，正好与input层进行全连接(相乘)得到D\*1的矩阵，采用线性映射将one-hot表示投影到稠密D维表示。

  ![68747470733a2f2f7778332e73696e61696d672e636e2f6c617267652f30303633304465666c7931673274317332306a706e6a333066313037353735692e6a7067](/Users/xiangjianhang/init-git/pigeonwx.github.io/docs/AI/nlp/68747470733a2f2f7778332e73696e61696d672e636e2f6c617267652f30303633304465666c7931673274317332306a706e6a333066313037353735692e6a7067.jpeg)

- output层(softmax)自然是前向窗中需要预测的词。

- 通过BP＋SGD得到最优的C投影矩阵，这就是NNLM的中间产物，也是我们所求的文本表示矩阵，**通过NNLM将稀疏矩阵投影到稠密向量矩阵中。**

#### Word2Vec

谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，**分别是CBOW（Continues Bag of Words）连续词袋和Skip-gram。**Word2Vec和上面的NNLM很类似，但比NNLM简单。

**CBOW**

CBOW获得中间词两边的的上下文，然后用周围的词去预测中间的词，把中间词当做y，把窗口中的其它词当做x输入，x输入是经过one-hot编码过的，然后通过一个隐层进行求和操作，最后通过激活函数softmax，可以计算出每个单词的生成概率，接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化，而求得的权重矩阵就是文本表示词向量的结果。

![1_PkYJEQhxzPabhHiuNj-s5Q](/Users/xiangjianhang/init-git/pigeonwx.github.io/docs/AI/nlp/1_PkYJEQhxzPabhHiuNj-s5Q.png)

**Skip-gram**：

Skip-gram是通过当前词来预测窗口中上下文词出现的概率模型，把当前词当做x，把窗口中其它词当做y，依然是通过一个隐层接一个Softmax激活函数来预测其它词的概率。如下图所示：

![68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d382d32305f32302d33342d302e6a7067](/Users/xiangjianhang/init-git/pigeonwx.github.io/docs/AI/nlp/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d382d32305f32302d33342d302e6a7067.jpeg)



**优化方法**：

> https://blog.csdn.net/BGoodHabit/article/details/106163130

- **层次Softmax：**至此还没有结束，因为如果单单只是接一个softmax激活函数，计算量还是很大的，有多少词就会有多少维的权重矩阵，所以这里就提出**层次Softmax(Hierarchical Softmax)**，使用Huffman Tree来编码输出层的词典，相当于平铺到各个叶子节点上，**瞬间把维度降低到了树的深度**，可以看如下图所示。这课Tree把出现频率高的词放到靠近根节点的叶子节点处，每一次只要做二分类计算，计算路径上所有非叶子节点词向量的贡献即可。

- **负例采样(Negative Sampling)：**这种优化方式做的事情是，在正确单词以外的负样本中进行采样，最终目的是为了减少负样本的数量，达到减少计算量效果。将词典中的每一个词对应一条线段，所有词组成了[0，1］间的剖分，如下图所示，然后每次随机生成一个[1, M-1]间的整数，看落在哪个词对应的剖分上就选择哪个词，最后会得到一个负样本集合。

**Word2Vec存在的问题**

- 对每个local context window单独训练，没有利用包 含在global co-currence矩阵中的统计信息。
- 对多义词无法很好的表示和处理，因为使用了唯一的词向量

#### sense2vec

Sense2Vec 是一种改进的词向量表示方法，它在原始的 Word2Vec 模型基础上增加了对词义（sense）的区分。传统的 Word2Vec 模型，如 Skip-gram 和 CBOW 模型，会将一个词表示为一个固定的向量，而不考虑词义的多样性。因此，对于那些具有多重含义的词，传统的 Word2Vec 模型无法很好地区分其不同的上下文含义。

Sense2Vec 通过引入词义区分，解决了一词多义问题，使得同一个词在不同的上下文中可以有不同的向量表示。这种改进在许多自然语言处理任务中，如词义消歧、文本分类和信息检索中，可以提供更为准确和细粒度的表示。



Sense2Vec 的核心思想是将一个词的不同词义作为不同的实体来处理。具体方法如下：

1. **预处理数据**：在训练过程中，首先对数据进行预处理，为每个词标记其词性（Part-of-Speech, POS）或者其他上下文信息。
2. **词义区分**：将标记好的词-词性对作为独立实体进行训练。例如，"apple/NOUN" 和 "apple/VERB" 对应于“苹果”和“采摘苹果”等不同的含义，将会有各自的向量表示。
3. **训练模型**：使用类似 Word2Vec 的算法（如 Skip-gram 或 CBOW）来训练词向量，每个词-词性对会分别得到其向量表示。

**示例**

假设我们有以下几个句子：

1. "I ate an apple."
2. "Apple Inc. is an American technology company."

在这里，"apple" 在第一个句子中是一个名词（NOUN），在第二个句子中是一个专有名词（PROPN，Proper Noun）。

在 Sense2Vec 中，这些将被标记为 `apple/NOUN` 和 `apple/PROPN`，并分别训练其向量表示。这样，尽管两个向量可能会有一定的相似性（因为它们共享一些上下文），但是仍然可以区分出它们不同的含义。

**优点**

1. **区分多义词**：可以根据上下文区分同一个词的不同含义，这是 Word2Vec 无法做到的。
2. **细粒度表示**：通过结合词性或上下文信息，可以获得更细粒度的词向量表示。
3. **增强语义理解**：在处理复杂文本时，能够更准确地理解和表示词语的语义。

**实现与库**

Sense2Vec 最早由 Reddit 开发，并在 "Bag of Words Meets Bags of Popcorn" 项目中取得了显著成果。其开源实现允许用户在自己的数据集上训练 Sense2Vec 模型。

在 Python 中，可以使用 Sense2Vec 库来实现该功能：

```python
from sense2vec import Sense2Vec

# 载入预训练的 Sense2Vec 模型
s2v = Sense2Vec().from_disk('/path/to/s2v/reddit_vectors-1.1.0')

# 查询一个词的向量表示和最相近的词
query = "apple|NOUN"
vector = s2v[query]

# 找到与 'apple|NOUN' 最相近的词
most_similar = s2v.most_similar(query, n=10)
for sense, score in most_similar:
    print(sense, score)
```

**总结**

Sense2Vec 是一种增强型的词向量表示方法，通过区分词的不同词义，提供了更细粒度和准确的表示。在很多 NLP 任务中，这种区别能够显著提高模型的性能和理解能力。如果你在处理自然语言处理问题时遇到了多义词的问题，Sense2Vec 可能是一个值得尝试的方法。

### 词嵌入为何不采用one-hot向量

虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，one-hot词向量⽆法准确表达不同词之间的相似度，如我们常常使⽤的余弦相似度。由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过onehot向量准确地体现出来。

word2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表⽰成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。



## fastText

英语单词通常有其内部结构和形成⽅式。例如，我们可以从“dog”“dogs”和“dogcatcher”的字⾯上推测它们的关系。这些词都有同⼀个词根“dog”，但使⽤不同的后缀来改变词的含义。而且，这个关联可以推⼴⾄其他词汇。

在word2vec中，我们并没有直接利⽤构词学中的信息。⽆论是在跳字模型还是连续词袋模型中，我们都将形态不同的单词⽤不同的向量来表⽰。例如，**“dog”和“dogs”分别⽤两个不同的向量表⽰，而模型中并未直接表达这两个向量之间的关系。鉴于此，fastText提出了⼦词嵌⼊(subword embedding)的⽅法，从而试图将构词信息引⼊word2vec中的CBOW。**

这里有一点需要特别注意，一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。除非你决定使用预训练的embedding来训练fastText分类模型，这另当别论。



### 总览

FastText 是一种用于词向量表示和文本分类的库，由 Facebook 的 AI 研究团队（FAIR）开发。与传统的 Word2Vec 等词向量表示方法相比，FastText 有几个显著的改进和优势，特别是在处理词汇表外词（Out-of-Vocabulary Words）和具有丰富形态变化的语言时。

**主要特点**

1. **子词信息**：FastText 将词分解为多个字符 n-gram，例如对于词 "where"，可能会分解为 "<wh", "whe", "her", "ere", "re>" 等 n-gram（<和>表示词的开始和结束）。
2. **高效**：FastText 可以在较短时间内处理大规模的数据集，并且支持多线程操作。
3. **应对罕见词和未登录词**：由于 FastText 使用了子词信息，它在处理罕见词和未登录词时表现更好——可以通过已有的子词信息生成词向量。
4. **文本分类**：FastText 同时支持高效的文本分类任务。

**FastText** 的模型

FastText 实际上有两种主要模型：

1. **词向量模型**：用于生成词向量。
2. **文本分类模型**：用于对文本进行分类。

**FastText** 词向量模型

FastText 使用与 Word2Vec 相似的 Skip-gram 或 CBOW 模型，但在计算词向量时，该模型不仅使用词本身，还使用词的所有 n-grams。例如：

- 对于 Skip-gram 模型，如果输入词是 "where"，模型将同时考虑 "where" 这个词本身以及其子词 "<wh", "whe", "her", "ere", "re>" 等。

这种方法有助于捕捉更多的词法形态信息，例如前缀、后缀和其它字符模式，从而提高表示罕见词或未登录词的能力。

### **FastText** 实现示例

以下是如何使用 FastText 库来训练词向量和进行文本分类的示例代码：

**安装** FastText

如果还没有安装 FastText，可以使用如下命令进行安装：

```sh
pip install fasttext
```

**训练词向量**

```python
import fasttext

# 训练词向量模型
model = fasttext.train_unsupervised('data.txt', model='skipgram')

# 获得词的向量表示
word_vector = model.get_word_vector('example')
print(word_vector)
```

**训练文本分类模型**

```python
import fasttext

# 这里的文本文件应该包含标签，格式如：__label__category text
model = fasttext.train_supervised('labeled_data.txt')

# 进行预测
label, confidence = model.predict('example text to classify')
print(label, confidence)
```

**优点**

1. **高效处理未登录词**：通过子词信息，FastText 能够有效地生成未曾见过的单词的向量表示。
2. **考虑词法形态**：特别适用于具有丰富词法形态变化的语言（例如阿拉伯语、芬兰语等）。
3. **高效**：FastText 实现了对大规模数据集的高效处理，并支持多线程加速。

**创建模型与使用案例**

FastText 的高效性和灵活性使其在各种实际场景中表现优异，例如：

- **情感分析**：对社交媒体、商品评论等进行情感分类。
- **文档分类**：自动化分类新闻文章、电子邮件等。
- **命名实体识别**：通过词向量生成，提高识别准确率。



### n-gram表示单词

word2vec把语料库中的每个单词当成原子的，它会为每个单词生成一个向量。这忽略了单词内部的形态特征，比如：“book” 和“books”，“阿里巴巴”和“阿里”，这两个例子中，两个单词都有较多公共字符，即它们的内部形态类似，但是在传统的word2vec中，这种单词内部形态信息因为它们被转换成不同的id丢失了。

**为了克服这个问题，fastText使用了字符级别的n-grams来表示一个单词。**对于单词“book”，假设n的取值为3，则它的trigram有:

**“<bo”,  “boo”,  “ook”, “ok>”**

其中，<表示前缀，>表示后缀。于是，我们可以用这些trigram来表示“book”这个单词，进一步，我们可以用这4个trigram的向量叠加来表示“apple”的词向量。

**这带来两点好处**：

1. 对于低频词生成的词向量效果会更好。因为它们的n-gram可以和其它词共享。
2. 对于训练词库之外的单词，仍然可以构建它们的词向量。我们可以叠加它们的字符级n-gram向量。



### fastText模型架构

之前提到过，fastText模型架构和word2vec的CBOW模型架构非常相似。下面是fastText模型架构图：

![68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d382d32315f32302d33312d32322e6a706567](/Users/xiangjianhang/init-git/pigeonwx.github.io/docs/AI/nlp/68747470733a2f2f67697465652e636f6d2f6b6b7765697368652f696d616765732f7261772f6d61737465722f4d4c2f323031392d382d32315f32302d33312d32322e6a706567.jpeg)

**注意**：此架构图没有展示词向量的训练过程。可以看到，和CBOW一样，fastText模型也只有三层：输入层、隐含层、输出层（Hierarchical Softmax），输入都是多个经向量表示的单词，输出都是一个特定的target，隐含层都是对多个词向量的叠加平均。

**不同的是，**

- CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；
- CBOW的输入单词被one-hot编码过，fastText的输入特征是被embedding过；
- CBOW的输出是目标词汇，fastText的输出是文档对应的类标。

**值得注意的是，fastText在输入时，将单词的字符级别的n-gram向量作为额外的特征；在输出时，fastText采用了分层Softmax，大大降低了模型训练时间。**这两个知识点在前文中已经讲过，这里不再赘述。

fastText相关公式的推导和CBOW非常类似，这里也不展开了。



### fastText核心思想



FastText 的核心思想是在生成词向量时考虑词的子词（subword）信息。具体地来说，FastText 将每个词分解成多个字符 n-gram，并将这些 n-gram 转换为向量。这样，即使一个词从未在训练数据中出现过，通过其子词构成，我们仍然可以生成一个合理的词向量。

现在抛开那些不是很讨人喜欢的公式推导，来想一想fastText文本分类的核心思想是什么？仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？**叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。**叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。

**于是fastText的核心思想就是：将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。**这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。



### 输出分类的效果

还有个问题，就是为何fastText的分类效果常常不输于传统的非线性分类器？

**假设我们有两段文本：**

肚子 饿了 我 要 吃饭

肚子 饿了 我 要 吃东西

这两段文本意思几乎一模一样，如果要分类，肯定要分到同一个类中去。但在传统的分类器中，用来表征这两段文本的向量可能差距非常大。传统的文本分类中，你需要计算出每个词的权重，比如TF-IDF值， “吃饭”和“吃东西” 算出的TF-IDF值相差可能会比较大，其它词类似，于是，VSM（向量空间模型）中用来表征这两段文本的文本向量差别可能比较大。

**但是fastText就不一样了，它是用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度**，于是，在fastText模型中，这两段文本的向量应该是非常相似的，于是，它们很大概率会被分到同一个类中。

使用词embedding而非词本身作为特征，这是fastText效果好的一个原因；另一个原因就是字符级n-gram特征的引入对分类效果会有一些提升 。



### fastText与Word2Vec的不同

有意思的是，fastText和Word2Vec的作者是同一个人。

**相同点**：

- 图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。
- 都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。

之前一直不明白fasttext用层次softmax时叶子节点是啥，CBOW很清楚，它的叶子节点是词和词频，后来看了源码才知道，其实fasttext叶子节点里是类标和类标的频数。

|      | Word2Vec                              | fastText                              |
| ---- | ------------------------------------- | ------------------------------------- |
| 输入 | one-hot形式的单词的向量               | embedding过的单词的词向量和n-gram向量 |
| 输出 | 对应的是每一个term,计算某term概率最大 | 对应的是分类的标签。                  |

**本质不同，体现在softmax的使用：**

word2vec的目的是得到词向量，该词向量最终是在输入层得到的，输出层对应的h-softmax也会生成一系列的向量，但是最终都被抛弃，不会使用。

fastText则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label

**fastText优点**：

1. **适合大型数据+高效的训练速度**：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”
2. **支持多语言表达**：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。FastText的性能要比时下流行的word2vec工具明显好上不少，也比其他目前最先进的词态词汇表征要好。
3. **专注于文本分类**，在许多标准问题上实现当下最好的表现（例如文本倾向性分析或标签预测）。







# 模型训练

## 正则化

### 正则化为什么能减少过拟合

在机器学习和深度学习中，过拟合是指模型在训练数据上表现非常好，但在测试数据或新数据上表现不佳的现象。这通常是因为模型学习到了训练数据中的噪音或特定模式，而这些模式并不能很好的泛化到未知数据。正则化是一种常用的技术，用于减少过拟合，从而提高模型的泛化能力。

以下是正则化能够减少过拟合的主要原因：

1. **惩罚复杂模型**：
   - 正则化技术通过增加一个惩罚项来限制模型的复杂性，这个惩罚项通常与模型参数的大小有关。
   - 一个复杂的模型可能会具有非常大的参数值，使得模型在训练数据上表现得很好，但在测试数据上表现不佳。
   - 通过惩罚过大的参数，正则化鼓励模型选择更加简单的、参数值较小的模型，从而减少过拟合。

2. **减少方差（Variance）**：
   - 在偏差-方差权衡中，正则化通过增加一点偏差来减少方差。高方差模型通常容易过拟合，因为它们对训练数据中的细节过于敏感。
   - 正则化方法可以平滑损失函数的形状，使模型不至于对训练数据中的微小变动产生剧烈响应。

3. **防止参数爆炸**：
   - 在一些优化过程中，参数值可能变得非常大，这会导致模型的预测不稳定。
   - 正则化项在损失函数中增加了一种形式的惩罚，使得参数倾向于保持在较小的范围内。

4. **平滑模型曲线**：
   - 在多项式回归等模型中，高阶项的系数通常会变得非常大，导致模型曲线过于弯曲，拟合了训练数据中的噪音。
   - 正则化可以有效地减少高阶系数的值，使得模型曲线平滑，从而更好地泛化到新数据。

### 常见正则化方法

1. **L2 正则化（Ridge 回归）**：
   - 在损失函数中添加参数的L2范数，即权重平方和的惩罚项。
   - 公式：`L2:  J = J + λ * Σ(w_i^2)`
   - L2 正则化会将参数尽可能缩减，同时保留许多参数。

2. **L1 正则化（Lasso 回归）**：
   - 在损失函数中添加参数的L1范数，即权重绝对值和的惩罚项。
   - 公式：`L1:  J = J + λ * Σ|w_i|`
   - L1 正则化会使一些参数变为零，从而进行特征选择。

3. **弹性网正则化（Elastic Net）**：
   - 将L1和L2正则化结合起来，以同时享有两者的优点。
   - 公式：`Elastic Net:  J = J + λ1 * Σ|w_i| + λ2 * Σ(w_i^2)`

4. **Dropout**（在神经网络中）：
   - 在每个训练周期中，以一定概率随机地将一些神经元“关闭”，即忽略其贡献。
   - 这种方法强制网络不得不在每次迭代中使用不同的子网络，从而减少过拟合。

5. **数据增强**：
   - 通过增加不同的训练实例来减少过拟合，例如在图像分类问题中，通过翻转、旋转、裁剪等技术增大训练数据集。

6. **早停（Early Stopping）**：
   - 在训练过程的某个阶段，如果发现验证集上的误差开始增加，就停止训练。
   - 早停可以防止模型在训练集上过拟合而在验证集或测试集上表现不佳。

通过正则化，可以有效地控制模型的复杂度，防止其过度拟合训练数据，从而提高模型在新数据上的泛化能力。



## 梯度问题

### 梯度消失



### 梯度爆炸



## 常见机器学习模型



## 常见深度学习模型

