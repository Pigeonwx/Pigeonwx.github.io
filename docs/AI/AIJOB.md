# 一、基础算法 常见面试篇
## 1.1 概率论

### 协方差矩阵的计算
[wikipedia](https://zh.wikipedia.org/wiki/%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5)

**面试官**：请解释一下什么是协方差矩阵，以及如何计算它？

**回答**：
协方差矩阵是一个用于描述多维随机变量之间关系的方阵。它的每个元素表示两个特征之间的协方差，反映了它们的线性关系。

计算协方差矩阵的步骤如下：

1. **去均值**：首先，我们需要计算每个特征的均值，并从每个特征中减去均值，以确保数据中心化。

2. **计算协方差**：然后，我们使用公式 \( \Sigma = \frac{1}{n-1} (X')^T X' \) 来计算协方差矩阵，其中 \( X' \) 是去均值后的数据矩阵。

3. **结果**：最终得到的协方差矩阵是一个 \( p \times p \) 的方阵，能够帮助我们理解特征之间的关系。

**面试官**：协方差矩阵有什么应用？

**回答**：
协方差矩阵在多个领域有广泛的应用，包括：

1. **主成分分析（PCA）**：通过计算协方差矩阵，我们可以找到数据的主成分，从而进行降维。

2. **金融风险管理**：在投资组合优化中，协方差矩阵用于评估不同资产之间的风险和收益关系。

3. **机器学习**：在高维数据分析中，协方差矩阵帮助我们理解特征之间的相关性，从而选择合适的特征进行建模。

4. **信号处理**：在信号处理领域，协方差矩阵用于分析信号的统计特性。


## 1.2 过拟合和欠拟合 常见面试篇

### 过拟合和欠拟合是什么？

**过拟合（Overfitting）**和**欠拟合（Underfitting）**是机器学习模型性能问题的两种常见表现形式。

- **过拟合**
  - 定义：模型在训练数据上表现很好，但在测试数据或真实环境中表现较差。此时，模型学到了训练数据中的噪声或细节，导致对新数据的泛化能力较差。
- **欠拟合**
  - 定义：模型在训练数据和测试数据上表现都不好。此时，模型过于简单，未能从数据中学到足够的规律。

### 过拟合/高方差（Overfitting / High Variance）篇

- **1 过拟合是什么及检验方法？**
  - **定义**：过拟合指模型在训练数据上表现优异，但在未见数据（如验证集或测试集）上误差很高，表现出较差的泛化能力。
  - 检验方法：
    - 验证误差显著高于训练误差：若训练集准确率接近 100%，但验证集准确率远低于训练集，则可能发生过拟合。
    - 学习曲线：观察训练误差和验证误差随样本数量增加的变化，若验证误差较高并未随数据量增大而减小，则可能过拟合。
- **2 导致过拟合的原因是什么？**
  - 模型过于复杂：模型参数过多，例如深层神经网络在小数据集上的应用。
  - 训练数据不足：数据量不足以支持复杂模型的训练。
  - 缺乏正则化：未对模型引入约束（如 L1 或 L2 正则化）以防止参数过大。
  - 数据噪声：数据本身存在噪声或异常点，模型被迫适应这些不必要的细节。
- **3 过拟合的解决方法是什么？**
  - 增加数据量：通过数据增强、采集新数据或生成合成数据，扩展数据集规模。
  - 使用正则化：通过 L1 或 L2 正则化增加模型的惩罚项，限制参数的绝对值大小。
  - 简化模型：降低模型复杂度，例如减少神经网络的层数或每层的神经元数量。
  - 使用交叉验证：帮助选择模型超参数，避免模型过度拟合特定验证集。
  - 提前停止：在训练过程中，监测验证集误差，当验证集误差不再下降时停止训练。
  - 使用 Dropout：在神经网络训练中随机丢弃部分神经元，以增强模型的鲁棒性。

### 欠拟合/高偏差（Underfitting / High Bias）篇

- **1 欠拟合是什么及检验方法？**
  - **定义**：欠拟合是指模型未能充分学习数据的特征，表现为训练误差和验证误差均较高。
  - 检验方法：
    - 训练误差高：如果模型在训练集上的误差就很高，说明模型未能很好地拟合数据。
    - 过于简单的假设：例如在非线性数据上使用线性模型，无法捕捉数据中的复杂模式。
- **2 导致欠拟合的原因是什么？**
  - 模型过于简单：模型的表达能力不足以捕捉数据中的复杂模式。
  - 训练不充分：模型未经过足够迭代训练，没有达到最优状态。
  - 数据特征不足：输入数据中缺乏有助于学习的特征或信息。
  - 高偏差算法：例如线性回归、简单决策树等无法很好地处理复杂数据。
- **3 欠拟合的解决方法是什么？**
  - 增加模型复杂度：选择更复杂的模型，如深度神经网络、支持向量机等。
  - 优化超参数：调整学习率、添加更多迭代次数，确保模型有充分的学习机会。
  - 增加数据特征：通过特征工程或使用非线性变换提升输入数据的质量。
  - 使用更灵活的算法：尝试更强大的模型或改进现有模型的结构。

---

### 正则化是什么

正则化（Regularization）

正则化是一种提高模型泛化能力的技术，旨在防止模型过拟合。通过对损失函数添加惩罚项（penalty term），限制模型的复杂度，正则化可以让模型在测试数据上的表现更加稳定。

**1. 为什么需要正则化？**

- 在机器学习中，模型可能会对训练数据拟合得太好，导致过拟合（overfitting），表现为在训练集上的误差较小，但在测试集上误差较大。
- 正则化通过限制模型参数的大小或约束模型的复杂性，有效地降低过拟合风险，提升模型的泛化能力。



**2. 正则化的常见方法**

**2.1 L1 正则化**

- **定义**
   L1 正则化通过在损失函数中加入所有参数绝对值的和作为惩罚项，从而约束参数大小。

- **公式**

  $L =  Loss + \lambda \sum_{i=1}^n |w_i|$

  其中，$\lambda$ 是正则化强度的超参数，$|w_i|$ 是参数的绝对值。

- **特点**

  - 能够使部分参数变为零，从而达到特征选择的效果。
  - 在稀疏模型中（如线性回归、稀疏编码）应用广泛。

**2.2 L2 正则化**

- **定义**
   L2 正则化通过在损失函数中加入所有参数平方和的惩罚项，来约束模型的参数。

- **公式**

  $L = Loss + \lambda \sum_{i=1}^n w_i^2$

  其中，λ\lambda 是正则化强度的超参数，wi2w_i^2 是参数的平方。

- **特点**

  - 通过减小权重值，使模型对特定特征的依赖性减弱，进而降低过拟合风险。
  - 与 L1 正则化相比，不会使参数为零，而是趋近于零。

**2.3 Elastic Net**

- **定义**
   Elastic Net 是 L1 和 L2 正则化的结合体，适用于高维数据场景。

- **公式**

  $L = Loss + \alpha \lambda \sum_{i=1}^n |w_i| + (1 - \alpha) \lambda \sum_{i=1}^n w_i^2$

  其中，α\alpha 是 L1 和 L2 正则化的权重调节参数。

- **特点**

  - 兼具 L1 正则化的特征选择能力和 L2 正则化的稳定性。
  - 常用于处理高维数据，如基因数据分析。

**3. 正则化的工作原理**

1. **限制模型参数**
    正则化通过约束参数的值（如缩小其绝对值或平方值），减少模型对训练数据的过度拟合。
2. **平衡损失函数**
    损失函数中加入正则化项，使模型在拟合训练数据的同时注重模型的简单性和稳定性。
3. **抑制复杂模型**
    减少参数数量或约束参数大小，防止模型变得过于复杂，从而降低对训练数据中的噪声和异常值的敏感性。

**4. 正则化的实际应用**

- **深度学习中的权重衰减**
   在深度学习中，L2 正则化常用于限制网络中权重的大小，称为权重衰减（Weight Decay）。
- **稀疏模型构建**
   L1 正则化被广泛应用于特征选择，特别是在高维数据中，通过稀疏化参数提升模型可解释性。
- **组合应用**
   Elastic Net 在文本分类和基因表达数据分析中表现良好，可同时处理多重共线性问题和特征选择。

**5. 正则化的注意事项**

1. **超参数调节**
    正则化强度由超参数 λ\lambda 控制，需通过交叉验证等方法找到最佳值。
2. **数据标准化**
    在使用正则化之前，建议对数据进行标准化（如 Z-score 标准化），以消除特征的量纲差异。
3. **过度正则化的风险**
    过强的正则化可能导致欠拟合，使模型无法充分学习数据特性。

**6. 正则化的优缺点**

- **优点**
  - 降低过拟合风险，提升模型的泛化能力。
  - 在高维数据中增强模型的鲁棒性。
- **缺点**
  - 增加了模型复杂度，训练时间可能变长。
  - 超参数调节需要额外的验证过程。

------

### 为什么 Dropout 可以增强模型的鲁棒性？

**Dropout** 是一种专门针对神经网络的正则化技术，通过在训练过程中随机地“丢弃”一部分神经元，防止模型过度依赖某些特定神经元，从而增强鲁棒性。

- **原理**：
  - 在每一次前向传播时，以一定的概率（如 50%）将部分神经元的输出设为零，使网络的部分结构被随机禁用。
  - 在测试阶段，保留所有神经元，但使用训练阶段的平均权重，从而形成一种“模型集成”的效果。
- **为什么增强鲁棒性？**
  1. **减少参数依赖**：随机丢弃神经元迫使模型不能完全依赖于某些特定的神经元，而是学会利用所有特征的组合，从而提高网络的泛化能力。
  2. **降低协同适应性**：多个神经元的共同作用可能导致过拟合，而 Dropout 通过随机破坏这种协同关系，提升了模型对数据分布变化的适应能力。
  3. **等效于模型集成**：由于 Dropout 在每次训练中随机禁用部分神经元，相当于训练了多个不同的子模型。最终测试时，这些子模型的预测会被集成，从而提升整体性能。
- **注意事项**：
  - Dropout 适用于深度学习模型，尤其是在全连接层中效果显著。
  - 在使用 Dropout 时，可能需要适当增大学习率或训练轮次，以补偿训练效率的下降。



---

## 1.2 BatchNorm vs LayerNorm 常见面试篇

### 1.2.1 动机篇

####  独立同分布（Independent and Identically Distributed）与白化

**独立同分布（i.i.d.）** 是统计学中的一个重要概念，指的是一组随机变量既相互独立又来自同一概率分布。换句话说，i.i.d. 的样本在统计特性上是相同的，且每个样本的取值不受其他样本的影响。

**白化（Whitening）** 是一种数据预处理技术，旨在消除数据中的相关性，使得数据的协方差矩阵变为单位矩阵。白化的过程通常包括去均值和缩放，使得数据在每个维度上具有相同的方差，并且各维度之间不再相关。白化可以帮助提高机器学习模型的性能，尤其是在处理高维数据时。

#### 内部协变量偏移（Internal Covariate Shift，ICS）

**内部协变量偏移（ICS）** 是指在训练深度学习模型时，随着模型参数的更新，网络中某一层的输入分布发生变化的现象。这种变化可能导致后续层的学习变得更加困难，因为它们需要不断适应新的输入分布。ICS 是深度学习中的一个重要问题，尤其是在使用小批量（mini-batch）训练时。

#### ICS问题带来的后果是什么？

ICS 问题可能带来以下后果：

1. **训练不稳定**：由于输入分布的变化，模型的训练过程可能变得不稳定，导致损失函数波动较大，难以收敛。

2. **收敛速度变慢**：模型需要更多的迭代次数才能收敛，因为每次参数更新后，后续层需要重新适应新的输入分布。

3. **过拟合风险增加**：由于模型在不同的输入分布上进行训练，可能导致模型在某些特定分布上过拟合，而在其他分布上表现不佳。

4. **需要更复杂的优化算法**：为了应对 ICS 问题，可能需要使用更复杂的优化算法或正则化技术，这增加了模型训练的复杂性。

5. **影响模型的泛化能力**：如果模型在训练过程中未能有效应对 ICS，可能会导致其在测试集上的表现不佳，影响模型的泛化能力。

为了解决 ICS 问题，研究者们提出了多种方法，例如批量归一化（Batch Normalization），它通过标准化每一层的输入来减轻 ICS 的影响，从而提高训练的稳定性和速度。

### 1.2.2 Normalization 篇
  - 2.1 Normalization 的通用框架与基本思想


### 1.2.3 Batch Normalization 篇
  - 3.1 Batch Normalization（纵向规范化）是什么？
  - 3.2 Batch Normalization（纵向规范化）存在什么问题？
  - 3.3 Batch Normalization（纵向规范化）适用的场景是什么？
  - 3.4 BatchNorm 存在什么问题？

### 1.2.4 Layer Normalization（横向规范化） 篇
  - 4.1 Layer Normalization（横向规范化）是什么？
  - 4.2 Layer Normalization（横向规范化）有什么用？

### 1.2.5 BN vs LN 篇

### 1.2.6 主流 Normalization 方法为什么有效？

> [点击查看答案](https://articles.zsxq.com/id_wbep87ht600b.html)

- [激活函数 常见面试篇](BasicAlgorithm/激活函数.md)
  - 一、动机篇
    - 1.1 为什么要有激活函数？
  - 二、激活函数介绍篇
    - 2.1 sigmoid 函数篇
      - 2.1.1 什么是 sigmoid 函数？
      - 2.1.2 为什么选 sigmoid 函数 作为激活函数？
      - 2.1.3 sigmoid 函数 有什么缺点？
    - 2.2 tanh 函数篇
      - 2.2.1 什么是 tanh 函数？
      - 2.2.2 为什么选 tanh 函数 作为激活函数？
      - 2.2.3 tanh 函数 有什么缺点？
    - 2.3 relu 函数篇
      - 2.3.1 什么是 relu 函数？
      - 2.3.2 为什么选 relu 函数 作为激活函数？
      - 2.3.3 relu 函数 有什么缺点？
  - 三、激活函数选择篇


- [正则化常见面试篇](https://articles.zsxq.com/id_g6mir08c0s8d.html)
  - 一、L0，L1，L2正则化 篇
    - 1.1 正则化 是什么？
    - 1.2 什么是 L0 正则化 ？
    - 1.3 什么是 L1 （稀疏规则算子 Lasso regularization）正则化 ？
    - 1.4 什么是 L2 正则化（岭回归 Ridge Regression 或者 权重衰减 Weight Decay）正则化 ？
  - 二、对比篇
    - 2.1 什么是结构风险最小化？
    - 2.2 从结构风险最小化的角度理解L1和L2正则化
    - 2.3 L1 vs L2
  - 三、dropout 篇
    - 3.1 什么是 dropout？
    - 3.2 dropout 在训练和测试过程中如何操作？
    - 3.3 dropout 如何防止过拟合?

> [点击查看答案](https://articles.zsxq.com/id_g6mir08c0s8d.html)

- [优化算法及函数 常见面试篇](https://articles.zsxq.com/id_hqd9p17b6afk.html)
  - 一、动机篇
    - 1.1 为什么需要 优化函数？
    - 1.2 优化函数的基本框架是什么?
  - 二、优化函数介绍篇
    - 2.1 梯度下降法是什么?
    - 2.2 随机梯度下降法是什么?
    - 2.3 Momentum 是什么?
    - 2.4 SGD with Nesterov Acceleration 是什么?
    - 2.5 Adagrad 是什么?
    - 2.6 RMSProp/AdaDelta 是什么？
    - 2.7 Adam 是什么?
    - 2.8 Nadam 是什么?
  - 三、优化函数学霸笔记篇

> [点击查看答案](https://articles.zsxq.com/id_hqd9p17b6afk.html)

- [归一化 常见面试篇](https://articles.zsxq.com/id_8iemf392t53n.html)
  - 一、动机篇
    - 1.1 为什么要归一化？
  - 二、介绍篇
    - 2.1  归一化 有 哪些方法？
    - 2.2  归一化 各方法 特点？
    - 2.3  归一化 的 意义？
  - 三、应用篇
    - 3.1 哪些机器学习算法 需要做 归一化？
    - 3.2 哪些机器学习算法 不需要做 归一化？

> [点击查看答案](https://articles.zsxq.com/id_8iemf392t53n.html)

- [判别式（discriminative）模型 vs. 生成式(generative)模型 常见面试篇](https://articles.zsxq.com/id_siv7mtg3573r.html)
  - 一、判别式模型篇
    - 1.1 什么是判别式模型？
    - 1.2 判别式模型是思路是什么？
    - 1.3 判别式模型的优点是什么？
  - 二、生成式模型篇
    - 2.1 什么是生成式模型？
    - 2.2 生成式模型是思路是什么？
    - 2.3 生成式模型的优点是什么？
    - 2.4 生成式模型的缺点是什么？

> [点击查看答案](