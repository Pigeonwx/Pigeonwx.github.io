## 一、基础算法 常见面试篇

## 1.1 过拟合和欠拟合 常见面试篇

### 过拟合和欠拟合是什么？

**过拟合（Overfitting）**和**欠拟合（Underfitting）**是机器学习模型性能问题的两种常见表现形式。

- **过拟合**
  - 定义：模型在训练数据上表现很好，但在测试数据或真实环境中表现较差。此时，模型学到了训练数据中的噪声或细节，导致对新数据的泛化能力较差。
- **欠拟合**
  - 定义：模型在训练数据和测试数据上表现都不好。此时，模型过于简单，未能从数据中学到足够的规律。

### 过拟合/高方差（Overfitting / High Variance）篇

- **2.1 过拟合是什么及检验方法？**
  - **定义**：过拟合指模型在训练数据上表现优异，但在未见数据（如验证集或测试集）上误差很高，表现出较差的泛化能力。
  - 检验方法：
    - 验证误差显著高于训练误差：若训练集准确率接近 100%，但验证集准确率远低于训练集，则可能发生过拟合。
    - 学习曲线：观察训练误差和验证误差随样本数量增加的变化，若验证误差较高并未随数据量增大而减小，则可能过拟合。
- **2.2 导致过拟合的原因是什么？**
  - 模型过于复杂：模型参数过多，例如深层神经网络在小数据集上的应用。
  - 训练数据不足：数据量不足以支持复杂模型的训练。
  - 缺乏正则化：未对模型引入约束（如 L1 或 L2 正则化）以防止参数过大。
  - 数据噪声：数据本身存在噪声或异常点，模型被迫适应这些不必要的细节。
- **2.3 过拟合的解决方法是什么？**
  - 增加数据量：通过数据增强、采集新数据或生成合成数据，扩展数据集规模。
  - 使用正则化：通过 L1 或 L2 正则化增加模型的惩罚项，限制参数的绝对值大小。
  - 简化模型：降低模型复杂度，例如减少神经网络的层数或每层的神经元数量。
  - 使用交叉验证：帮助选择模型超参数，避免模型过度拟合特定验证集。
  - 提前停止：在训练过程中，监测验证集误差，当验证集误差不再下降时停止训练。
  - 使用 Dropout：在神经网络训练中随机丢弃部分神经元，以增强模型的鲁棒性。

### 欠拟合/高偏差（Underfitting / High Bias）篇

- **3.1 欠拟合是什么及检验方法？**
  - **定义**：欠拟合是指模型未能充分学习数据的特征，表现为训练误差和验证误差均较高。
  - 检验方法：
    - 训练误差高：如果模型在训练集上的误差就很高，说明模型未能很好地拟合数据。
    - 过于简单的假设：例如在非线性数据上使用线性模型，无法捕捉数据中的复杂模式。
- **3.2 导致欠拟合的原因是什么？**
  - 模型过于简单：模型的表达能力不足以捕捉数据中的复杂模式。
  - 训练不充分：模型未经过足够迭代训练，没有达到最优状态。
  - 数据特征不足：输入数据中缺乏有助于学习的特征或信息。
  - 高偏差算法：例如线性回归、简单决策树等无法很好地处理复杂数据。
- **3.3 欠拟合的解决方法是什么？**
  - 增加模型复杂度：选择更复杂的模型，如深度神经网络、支持向量机等。
  - 优化超参数：调整学习率、添加更多迭代次数，确保模型有充分的学习机会。
  - 增加数据特征：通过特征工程或使用非线性变换提升输入数据的质量。
  - 使用更灵活的算法：尝试更强大的模型或改进现有模型的结构。

---

### 正则化是什么

正则化（Regularization）

正则化是一种提高模型泛化能力的技术，旨在防止模型过拟合。通过对损失函数添加惩罚项（penalty term），限制模型的复杂度，正则化可以让模型在测试数据上的表现更加稳定。

**1. 为什么需要正则化？**

- 在机器学习中，模型可能会对训练数据拟合得太好，导致过拟合（overfitting），表现为在训练集上的误差较小，但在测试集上误差较大。
- 正则化通过限制模型参数的大小或约束模型的复杂性，有效地降低过拟合风险，提升模型的泛化能力。



**2. 正则化的常见方法**

**2.1 L1 正则化**

- **定义**
   L1 正则化通过在损失函数中加入所有参数绝对值的和作为惩罚项，从而约束参数大小。

- **公式**

  L=Loss+λ∑i=1n∣wi∣L = Loss + \lambda \sum_{i=1}^n |w_i|

  其中，λ\lambda 是正则化强度的超参数，∣wi∣|w_i| 是参数的绝对值。

- **特点**

  - 能够使部分参数变为零，从而达到特征选择的效果。
  - 在稀疏模型中（如线性回归、稀疏编码）应用广泛。

**2.2 L2 正则化**

- **定义**
   L2 正则化通过在损失函数中加入所有参数平方和的惩罚项，来约束模型的参数。

- **公式**

  L=Loss+λ∑i=1nwi2L = Loss + \lambda \sum_{i=1}^n w_i^2

  其中，λ\lambda 是正则化强度的超参数，wi2w_i^2 是参数的平方。

- **特点**

  - 通过减小权重值，使模型对特定特征的依赖性减弱，进而降低过拟合风险。
  - 与 L1 正则化相比，不会使参数为零，而是趋近于零。

**2.3 Elastic Net**

- **定义**
   Elastic Net 是 L1 和 L2 正则化的结合体，适用于高维数据场景。

- **公式**

  L=Loss+αλ∑i=1n∣wi∣+(1−α)λ∑i=1nwi2L = Loss + \alpha \lambda \sum_{i=1}^n |w_i| + (1 - \alpha) \lambda \sum_{i=1}^n w_i^2

  其中，α\alpha 是 L1 和 L2 正则化的权重调节参数。

- **特点**

  - 兼具 L1 正则化的特征选择能力和 L2 正则化的稳定性。
  - 常用于处理高维数据，如基因数据分析。

**3. 正则化的工作原理**

1. **限制模型参数**
    正则化通过约束参数的值（如缩小其绝对值或平方值），减少模型对训练数据的过度拟合。
2. **平衡损失函数**
    损失函数中加入正则化项，使模型在拟合训练数据的同时注重模型的简单性和稳定性。
3. **抑制复杂模型**
    减少参数数量或约束参数大小，防止模型变得过于复杂，从而降低对训练数据中的噪声和异常值的敏感性。

**4. 正则化的实际应用**

- **深度学习中的权重衰减**
   在深度学习中，L2 正则化常用于限制网络中权重的大小，称为权重衰减（Weight Decay）。
- **稀疏模型构建**
   L1 正则化被广泛应用于特征选择，特别是在高维数据中，通过稀疏化参数提升模型可解释性。
- **组合应用**
   Elastic Net 在文本分类和基因表达数据分析中表现良好，可同时处理多重共线性问题和特征选择。

**5. 正则化的注意事项**

1. **超参数调节**
    正则化强度由超参数 λ\lambda 控制，需通过交叉验证等方法找到最佳值。
2. **数据标准化**
    在使用正则化之前，建议对数据进行标准化（如 Z-score 标准化），以消除特征的量纲差异。
3. **过度正则化的风险**
    过强的正则化可能导致欠拟合，使模型无法充分学习数据特性。

**6. 正则化的优缺点**

- **优点**
  - 降低过拟合风险，提升模型的泛化能力。
  - 在高维数据中增强模型的鲁棒性。
- **缺点**
  - 增加了模型复杂度，训练时间可能变长。
  - 超参数调节需要额外的验证过程。

------

### 为什么 Dropout 可以增强模型的鲁棒性？

**Dropout** 是一种专门针对神经网络的正则化技术，通过在训练过程中随机地“丢弃”一部分神经元，防止模型过度依赖某些特定神经元，从而增强鲁棒性。

- **原理**：
  - 在每一次前向传播时，以一定的概率（如 50%）将部分神经元的输出设为零，使网络的部分结构被随机禁用。
  - 在测试阶段，保留所有神经元，但使用训练阶段的平均权重，从而形成一种“模型集成”的效果。
- **为什么增强鲁棒性？**
  1. **减少参数依赖**：随机丢弃神经元迫使模型不能完全依赖于某些特定的神经元，而是学会利用所有特征的组合，从而提高网络的泛化能力。
  2. **降低协同适应性**：多个神经元的共同作用可能导致过拟合，而 Dropout 通过随机破坏这种协同关系，提升了模型对数据分布变化的适应能力。
  3. **等效于模型集成**：由于 Dropout 在每次训练中随机禁用部分神经元，相当于训练了多个不同的子模型。最终测试时，这些子模型的预测会被集成，从而提升整体性能。
- **注意事项**：
  - Dropout 适用于深度学习模型，尤其是在全连接层中效果显著。
  - 在使用 Dropout 时，可能需要适当增大学习率或训练轮次，以补偿训练效率的下降。



---

### BatchNorm vs LayerNorm 常见面试篇

- 一、动机篇
  - 1.1 独立同分布（independent and identically distributed）与白化
  - 1.2 （ Internal Covariate Shift，ICS）
  - 1.3 ICS问题带来的后果是什么？
- 二、Normalization 篇
  - 2.1 Normalization 的通用框架与基本思想
- 三、Batch Normalization 篇
  - 3.1 Batch Normalization（纵向规范化）是什么？
  - 3.2 Batch Normalization（纵向规范化）存在什么问题？
  - 3.3 Batch Normalization（纵向规范化）适用的场景是什么？
  - 3.4 BatchNorm 存在什么问题？
- 四、Layer Normalization（横向规范化） 篇
  - 4.1 Layer Normalization（横向规范化）是什么？
  - 4.2 Layer Normalization（横向规范化）有什么用？
- 五、BN vs LN 篇
- 六、主流 Normalization 方法为什么有效？

> [点击查看答案](https://articles.zsxq.com/id_wbep87ht600b.html)

- [激活函数 常见面试篇](BasicAlgorithm/激活函数.md)
  - 一、动机篇
    - 1.1 为什么要有激活函数？
  - 二、激活函数介绍篇
    - 2.1 sigmoid 函数篇
      - 2.1.1 什么是 sigmoid 函数？
      - 2.1.2 为什么选 sigmoid 函数 作为激活函数？
      - 2.1.3 sigmoid 函数 有什么缺点？
    - 2.2 tanh 函数篇
      - 2.2.1 什么是 tanh 函数？
      - 2.2.2 为什么选 tanh 函数 作为激活函数？
      - 2.2.3 tanh 函数 有什么缺点？
    - 2.3 relu 函数篇
      - 2.3.1 什么是 relu 函数？
      - 2.3.2 为什么选 relu 函数 作为激活函数？
      - 2.3.3 relu 函数 有什么缺点？
  - 三、激活函数选择篇


- [正则化常见面试篇](https://articles.zsxq.com/id_g6mir08c0s8d.html)
  - 一、L0，L1，L2正则化 篇
    - 1.1 正则化 是什么？
    - 1.2 什么是 L0 正则化 ？
    - 1.3 什么是 L1 （稀疏规则算子 Lasso regularization）正则化 ？
    - 1.4 什么是 L2 正则化（岭回归 Ridge Regression 或者 权重衰减 Weight Decay）正则化 ？
  - 二、对比篇
    - 2.1 什么是结构风险最小化？
    - 2.2 从结构风险最小化的角度理解L1和L2正则化
    - 2.3 L1 vs L2
  - 三、dropout 篇
    - 3.1 什么是 dropout？
    - 3.2 dropout 在训练和测试过程中如何操作？
    - 3.3 dropout 如何防止过拟合?

> [点击查看答案](https://articles.zsxq.com/id_g6mir08c0s8d.html)

- [优化算法及函数 常见面试篇](https://articles.zsxq.com/id_hqd9p17b6afk.html)
  - 一、动机篇
    - 1.1 为什么需要 优化函数？
    - 1.2 优化函数的基本框架是什么?
  - 二、优化函数介绍篇
    - 2.1 梯度下降法是什么?
    - 2.2 随机梯度下降法是什么?
    - 2.3 Momentum 是什么?
    - 2.4 SGD with Nesterov Acceleration 是什么?
    - 2.5 Adagrad 是什么?
    - 2.6 RMSProp/AdaDelta 是什么？
    - 2.7 Adam 是什么?
    - 2.8 Nadam 是什么?
  - 三、优化函数学霸笔记篇

> [点击查看答案](https://articles.zsxq.com/id_hqd9p17b6afk.html)

- [归一化 常见面试篇](https://articles.zsxq.com/id_8iemf392t53n.html)
  - 一、动机篇
    - 1.1 为什么要归一化？
  - 二、介绍篇
    - 2.1  归一化 有 哪些方法？
    - 2.2  归一化 各方法 特点？
    - 2.3  归一化 的 意义？
  - 三、应用篇
    - 3.1 哪些机器学习算法 需要做 归一化？
    - 3.2 哪些机器学习算法 不需要做 归一化？

> [点击查看答案](https://articles.zsxq.com/id_8iemf392t53n.html)

- [判别式（discriminative）模型 vs. 生成式(generative)模型 常见面试篇](https://articles.zsxq.com/id_siv7mtg3573r.html)
  - 一、判别式模型篇
    - 1.1 什么是判别式模型？
    - 1.2 判别式模型是思路是什么？
    - 1.3 判别式模型的优点是什么？
  - 二、生成式模型篇
    - 2.1 什么是生成式模型？
    - 2.2 生成式模型是思路是什么？
    - 2.3 生成式模型的优点是什么？
    - 2.4 生成式模型的缺点是什么？

> [点击查看答案](